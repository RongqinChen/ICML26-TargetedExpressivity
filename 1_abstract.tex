\begin{abstract}
Graph Neural Networks (GNNs) are fundamental for learning on graph-structured data, yet their expressive power is inherently bounded by the 1-Weisfeiler-Lehman (1-WL) isomorphism test. While higher-order \(k\)-FWL GNNs overcome this limitation, their prohibitive combinatorial complexity creates a critical expressivity–efficiency trade-off. To address this, we present a novel theoretical framework that reinterprets \(k\)-FWL through the lens of abstract simplicial complexes. This perspective reveals that the \(k\)-FWL algorithm operates on the \(k\)-skeleton of a complex built from the graph, where higher-order simplices correspond to potential redundant computations.
Our core contribution is a principled characterization of when lower-order models suffice and how to prune redundant high-order interactions without loss of expressivity. First, we prove that for any graph family lacking \((k+1)\)-cohesive vertex sets—a novel structural invariant based on pairwise connectivity—\(k\)-FWL provides a complete discriminative basis. Second, we introduce the *pruned \(k\)-FWL GNN*, which operates selectively on the sub-complex of cohesive simplices, and formally demonstrate its equivalence in expressive power to the full \(k\)-FWL. Third, we establish that non-cohesive simplices are universally reducible across the \(k\)-FWL hierarchy. Collectively, these results offer a rigorous framework for understanding minimal sufficient expressivity and provide a theoretically grounded strategy to enhance the scalability of higher-order graph neural architectures.
\end{abstract}
